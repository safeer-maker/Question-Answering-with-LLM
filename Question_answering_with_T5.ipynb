{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Create a question answer section using T5 or other model on eli5 dataset.\n",
    "\n",
    "If we use smaller llm models they can give the answer of the question but failed to give answer in human manner.\n",
    "I have pre tained the bert model but it will only provide the extraceted asnwer. By extracted it mean the exact words will be generated againt the question.\n",
    "\n",
    "Lets make it more fun. I think to use FIAS sementic serach for extraction relavent documents ffrom the model and then provide the text to the model for proper human answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Safeer\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 18.2k/18.2k [00:00<?, ?B/s]\n",
      "Downloading metadata: 100%|██████████| 6.36k/6.36k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 15.8k/15.8k [00:00<00:00, 15.9MB/s]\n",
      "Downloading: 100%|██████████| 3.50k/3.50k [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 576M/576M [01:09<00:00, 8.32MB/s]  \n",
      "Downloading: 100%|██████████| 21.1M/21.1M [00:03<00:00, 5.75MB/s]\n",
      "Downloading: 100%|██████████| 53.0M/53.0M [00:08<00:00, 6.57MB/s]\n",
      "Downloading: 100%|██████████| 286M/286M [00:36<00:00, 7.86MB/s] \n",
      "Downloading: 100%|██████████| 9.65M/9.65M [00:02<00:00, 4.31MB/s]\n",
      "Downloading: 100%|██████████| 17.7M/17.7M [00:03<00:00, 5.17MB/s]\n",
      "Downloading: 100%|██████████| 330M/330M [00:41<00:00, 7.89MB/s] \n",
      "Downloading: 100%|██████████| 18.7M/18.7M [00:03<00:00, 4.80MB/s]\n",
      "Downloading: 100%|██████████| 36.2M/36.2M [00:06<00:00, 5.95MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_eli5: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 272634\n",
       "    })\n",
       "    validation_eli5: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 9812\n",
       "    })\n",
       "    test_eli5: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 24512\n",
       "    })\n",
       "    train_asks: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 131778\n",
       "    })\n",
       "    validation_asks: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 2281\n",
       "    })\n",
       "    test_asks: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 4462\n",
       "    })\n",
       "    train_askh: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 98525\n",
       "    })\n",
       "    validation_askh: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 4901\n",
       "    })\n",
       "    test_askh: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "        num_rows: 9764\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"eli5\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets combine the dataset as we only need to answer the question on all the possible topic we can have\n",
    "\n",
    "WE are not going to train our model on this dataset. Insted we need to use this information for our sementic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_eli5',\n",
       " 'validation_eli5',\n",
       " 'test_eli5',\n",
       " 'train_asks',\n",
       " 'validation_asks',\n",
       " 'test_asks',\n",
       " 'train_askh',\n",
       " 'validation_askh',\n",
       " 'test_askh']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_titles = list (dataset.keys() )\n",
    "dataset_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers', 'title_urls', 'selftext_urls', 'answers_urls'],\n",
       "    num_rows: 4462\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try to add test and train data together\n",
    "dataset['test_asks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
